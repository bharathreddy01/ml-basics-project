{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4fa0b7b2-defc-48c9-a9ab-7f147bd2e6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found dataset: PlantVillage at /Volumes/RAY/CropMoiniteringdata/Healthy crop /PlantVillage\n",
      "âœ… Found dataset: CCMT at /Volumes/RAY/CropMoiniteringdata/CCMT_FInal Dataset\n",
      "âœ… Found dataset: PlantDoc at /Volumes/RAY/CropMoiniteringdata/PlantDoc-Dataset/train\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the selected dataset paths\n",
    "datasets = {\n",
    "    \"PlantVillage\": r\"/Volumes/RAY/CropMoiniteringdata/Healthy crop /PlantVillage\",\n",
    "    \"CCMT\": r\"/Volumes/RAY/CropMoiniteringdata/CCMT_FInal Dataset\",\n",
    "    \"PlantDoc\": r\"/Volumes/RAY/CropMoiniteringdata/PlantDoc-Dataset/train\"\n",
    "}\n",
    "\n",
    "# Check if each dataset path exists\n",
    "for dataset_name, dataset_path in datasets.items():\n",
    "    if os.path.exists(dataset_path):\n",
    "        print(f\"âœ… Found dataset: {dataset_name} at {dataset_path}\")\n",
    "    else:\n",
    "        print(f\"âŒ ERROR: Dataset not found: {dataset_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2e4569be-51a5-4912-8bae-67e925881837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ PlantVillage: 11874 images found\n",
      "ğŸ“‚ CCMT: 18697 images found\n",
      "ğŸ“‚ PlantDoc: 2316 images found\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Function to count images in each dataset folder\n",
    "def count_images(folder_path):\n",
    "    total_images = 0\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        total_images += len([f for f in files if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.gif'))])\n",
    "    return total_images\n",
    "\n",
    "# Check image counts for each dataset\n",
    "for dataset_name, dataset_path in datasets.items():\n",
    "    if os.path.exists(dataset_path):\n",
    "        num_images = count_images(dataset_path)\n",
    "        print(f\"ğŸ“‚ {dataset_name}: {num_images} images found\")\n",
    "    else:\n",
    "        print(f\"âŒ ERROR: Dataset folder not found - {dataset_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3927986f-a410-4586-ae5b-7f95bd5fd642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created category folders inside: CropMonitoringData_Sorted\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the target directory for sorted images\n",
    "target_base_dir = \"CropMonitoringData_Sorted\"\n",
    "\n",
    "# Define categories\n",
    "categories = [\"Healthy\", \"Diseased\", \"Pest\", \"Nutrient_Deficiency\", \"Moisture_Stress\"]\n",
    "\n",
    "# Ensure each category folder exists\n",
    "for category in categories:\n",
    "    os.makedirs(os.path.join(target_base_dir, category), exist_ok=True)\n",
    "\n",
    "print(f\"âœ… Created category folders inside: {target_base_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b26eb020-e5f7-442a-adbf-8f30a71dad49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Moving images from: PlantVillage\n",
      "âœ… Moving images from: CCMT\n",
      "âœ… Moving images from: PlantDoc\n",
      "ğŸ‰ All images have been successfully organized into categories!\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Mapping dataset folder names to our categories\n",
    "folder_mapping = {\n",
    "    # Healthy Crops\n",
    "    \"Tomato_healthy\": \"Healthy\",\n",
    "    \"Pepper__bell___healthy\": \"Healthy\",\n",
    "\n",
    "    # Diseased Crops\n",
    "    \"Tomato_Bacterial_spot\": \"Diseased\",\n",
    "    \"Tomato_Early_blight\": \"Diseased\",\n",
    "    \"Tomato_Late_blight\": \"Diseased\",\n",
    "    \"Tomato_Septoria_leaf_spot\": \"Diseased\",\n",
    "    \"Tomato_Target_Spot\": \"Diseased\",\n",
    "    \"Tomato_Tomato_YellowLeaf_Curl_Virus\": \"Diseased\",\n",
    "    \"Tomato_Tomato_mosaic_virus\": \"Diseased\",\n",
    "    \"Potato_Early_blight\": \"Diseased\",\n",
    "    \"Potato_Late_blight\": \"Diseased\",\n",
    "    \"Pepper__bell___Bacterial_spot\": \"Diseased\",\n",
    "\n",
    "    # Pest Infestation\n",
    "    \"Tomato_Spider_mites_Two_spotted_spider_mite\": \"Pest\",\n",
    "\n",
    "    # Nutrient Deficiency (Need Verification)\n",
    "    \"bacterial_leaf_blight\": \"Nutrient_Deficiency\",\n",
    "    \"bacterial_leaf_streak\": \"Nutrient_Deficiency\",\n",
    "    \"bacterial_panicle_blight\": \"Nutrient_Deficiency\",\n",
    "\n",
    "    # Moisture Stress\n",
    "    \"dead_heart\": \"Moisture_Stress\",\n",
    "    \"downy_mildew\": \"Moisture_Stress\",\n",
    "    \"blast\": \"Moisture_Stress\",\n",
    "    \"brown_spot\": \"Moisture_Stress\",\n",
    "    \"hispa\": \"Moisture_Stress\",\n",
    "    \"tungro\": \"Moisture_Stress\"\n",
    "}\n",
    "\n",
    "# Function to move images from dataset folders to the correct category\n",
    "def move_images(source_folder, mapping):\n",
    "    for root, dirs, files in os.walk(source_folder):  # Recursively find all files\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.gif')):  # Only move valid images\n",
    "                folder_name = os.path.basename(root)  # Get last folder name\n",
    "                category = mapping.get(folder_name, None)\n",
    "\n",
    "                if category:\n",
    "                    src_file_path = os.path.join(root, file)\n",
    "                    dest_folder_path = os.path.join(target_base_dir, category)\n",
    "\n",
    "                    os.makedirs(dest_folder_path, exist_ok=True)  # Ensure category folder exists\n",
    "                    shutil.move(src_file_path, os.path.join(dest_folder_path, file))\n",
    "\n",
    "# Move images from the selected datasets\n",
    "for dataset_name, dataset_path in datasets.items():\n",
    "    if os.path.exists(dataset_path):\n",
    "        print(f\"âœ… Moving images from: {dataset_name}\")\n",
    "        move_images(dataset_path, folder_mapping)\n",
    "    else:\n",
    "        print(f\"âŒ Dataset not found: {dataset_name}\")\n",
    "\n",
    "print(\"ğŸ‰ All images have been successfully organized into categories!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f83c9836-4d4a-4078-9cfe-8cee9d8f47d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Diseased: 0 images\n",
      "ğŸ“‚ Pest: 0 images\n",
      "ğŸ“‚ Healthy: 0 images\n",
      "ğŸ“‚ Moisture_Stress: 0 images\n",
      "ğŸ“‚ Nutrient_Deficiency: 0 images\n",
      "ğŸ“‚ train: 4 images\n",
      "ğŸ“‚ val: 4 images\n"
     ]
    }
   ],
   "source": [
    "for category in os.listdir(target_base_dir):\n",
    "    category_path = os.path.join(target_base_dir, category)\n",
    "    \n",
    "    if os.path.isdir(category_path):  # Ensure it's a valid directory\n",
    "        num_images = len(os.listdir(category_path))\n",
    "        print(f\"ğŸ“‚ {category}: {num_images} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6cb91e24-d929-4f91-b4e1-fc4160cc42ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“‚ Checking dataset: PlantVillage\n",
      "\n",
      "ğŸ“‚ Checking dataset: CCMT\n",
      "\n",
      "ğŸ“‚ Checking dataset: PlantDoc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check for Moisture_Stress and Nutrient_Deficiency images in the original datasets\n",
    "missing_categories = [\"Moisture_Stress\", \"Nutrient_Deficiency\"]\n",
    "\n",
    "for dataset_name, dataset_path in datasets.items():\n",
    "    print(f\"\\nğŸ“‚ Checking dataset: {dataset_name}\")\n",
    "    \n",
    "    if os.path.exists(dataset_path):\n",
    "        for folder in os.listdir(dataset_path):\n",
    "            folder_path = os.path.join(dataset_path, folder)\n",
    "\n",
    "            if os.path.isdir(folder_path) and folder in missing_categories:  \n",
    "                num_images = len([f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))])\n",
    "                print(f\"   - {folder}: {num_images} images\")\n",
    "    else:\n",
    "        print(f\"âŒ ERROR: Dataset folder not found - {dataset_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bc958c42-5f98-4349-8aa7-92d290a8d918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—‘ï¸ Removed empty folder: CropMonitoringData_Sorted/Moisture_Stress\n",
      "ğŸ—‘ï¸ Removed empty folder: CropMonitoringData_Sorted/Nutrient_Deficiency\n",
      "âœ… Only Healthy, Diseased, and Pest categories remain.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Define the categories to remove\n",
    "empty_categories = [\"Moisture_Stress\", \"Nutrient_Deficiency\"]\n",
    "\n",
    "# Remove empty folders\n",
    "for category in empty_categories:\n",
    "    category_path = os.path.join(target_base_dir, category)\n",
    "    if os.path.exists(category_path):\n",
    "        shutil.rmtree(category_path)\n",
    "        print(f\"ğŸ—‘ï¸ Removed empty folder: {category_path}\")\n",
    "\n",
    "print(\"âœ… Only Healthy, Diseased, and Pest categories remain.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ae7e1d59-6f06-4e40-aa5f-9fe905fef281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Automatically use MPS if available, otherwise fallback to CPU\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b75172a0-bd95-43f3-a467-711839a0f573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "âœ… Model moved to: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# âœ… Set device to MPS (Mac GPU) if available\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# âœ… Load a pre-trained ResNet18 model\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# âœ… Modify the last layer to match your number of classes\n",
    "num_classes = 2  # Change this based on your dataset (e.g., Healthy & Diseased)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# âœ… Move the model to MPS (Mac GPU)\n",
    "model = model.to(device)\n",
    "print(\"âœ… Model moved to:\", device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cb10ac9a-047b-4920-b2c9-2ec867eb6b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… CropMonitoringData_Sorted exists.\n",
      "ğŸ“‚ Contents: ['Diseased', 'Pest', '.DS_Store', 'Healthy', 'train', 'val']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base_dir = \"CropMonitoringData_Sorted\"\n",
    "\n",
    "if os.path.exists(base_dir):\n",
    "    print(f\"âœ… {base_dir} exists.\")\n",
    "    print(\"ğŸ“‚ Contents:\", os.listdir(base_dir))\n",
    "else:\n",
    "    print(f\"âŒ ERROR: {base_dir} not found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "51219a17-226f-48d5-95fe-07f651e0e1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Images successfully moved into 'train/' and 'val/' folders.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Define paths\n",
    "base_dir = \"CropMonitoringData_Sorted\"\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "val_dir = os.path.join(base_dir, \"val\")\n",
    "\n",
    "# Create train and val directories\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "# Categories to process (Now including 'Pest')\n",
    "categories = [\"Diseased\", \"Healthy\", \"Pest\"]\n",
    "\n",
    "for category in categories:\n",
    "    src_folder = os.path.join(base_dir, category)  # Original dataset folder\n",
    "    train_dest = os.path.join(train_dir, category)  # Train folder\n",
    "    val_dest = os.path.join(val_dir, category)  # Validation folder\n",
    "\n",
    "    os.makedirs(train_dest, exist_ok=True)\n",
    "    os.makedirs(val_dest, exist_ok=True)\n",
    "\n",
    "    images = [f for f in os.listdir(src_folder) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    random.shuffle(images)  # Shuffle images to ensure randomness\n",
    "\n",
    "    split_idx = int(len(images) * 0.8)  # 80% Train, 20% Validation\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        src_path = os.path.join(src_folder, img)\n",
    "        dest_path = os.path.join(train_dest if i < split_idx else val_dest, img)\n",
    "        shutil.move(src_path, dest_path)\n",
    "\n",
    "print(\"âœ… Images successfully moved into 'train/' and 'val/' folders.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4e078036-baf2-43bd-a518-882942c4c871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ ['Diseased', 'Pest', '.DS_Store', 'Healthy', 'train', 'val']\n",
      "ğŸ“‚ Train: ['Diseased', 'Pest', '.DS_Store', 'Healthy']\n",
      "ğŸ“‚ Val: ['Diseased', 'Pest', '.DS_Store', 'Healthy']\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“‚\", os.listdir(\"CropMonitoringData_Sorted\"))\n",
    "print(\"ğŸ“‚ Train:\", os.listdir(\"CropMonitoringData_Sorted/train\"))\n",
    "print(\"ğŸ“‚ Val:\", os.listdir(\"CropMonitoringData_Sorted/val\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "09df95cc-5f51-4a11-a13a-374eb1c686f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training Images: 12135\n",
      "âœ… Validation Images: 3036\n",
      "âœ… Classes: ['Diseased', 'Healthy', 'Pest']\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# âœ… Load datasets\n",
    "train_dataset = datasets.ImageFolder(root=\"CropMonitoringData_Sorted/train\", transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=\"CropMonitoringData_Sorted/val\", transform=transform)\n",
    "\n",
    "print(f\"âœ… Training Images: {len(train_dataset)}\")\n",
    "print(f\"âœ… Validation Images: {len(val_dataset)}\")\n",
    "print(f\"âœ… Classes: {train_dataset.classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "84a37e9b-94d3-452f-8145-074466d9cae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Train DataLoader: 380 batches\n",
      "âœ… Val DataLoader: 95 batches\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32\n",
    "\n",
    "# âœ… Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"âœ… Train DataLoader: {len(train_loader)} batches\")\n",
    "print(f\"âœ… Val DataLoader: {len(val_loader)} batches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "609e12ce-a5a2-485a-90ca-ad9a46b2b4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model moved to: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# âœ… Load Pretrained ResNet18\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")  # Use MPS on Mac\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Modify the final layer for 3 classes (Diseased, Healthy, Pest)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 3)  # 3 output classes\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "print(f\"âœ… Model moved to: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "94ffc8c6-1ab2-41e0-a72d-4f1dccbf3eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model ready for training!\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# âœ… Define Loss Function & Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"âœ… Model ready for training!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b20e1d56-5a05-4150-940d-80911c300821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Train DataLoader: 380 batches\n",
      "âœ… Val DataLoader: 95 batches\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# âœ… Define transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# âœ… Custom Dataset Class with Error Handling\n",
    "class CustomImageDataset(ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            path, target = self.samples[index]\n",
    "            image = Image.open(path).convert(\"RGB\")  # Convert to RGB to avoid mode issues\n",
    "            image = self.transform(image)\n",
    "            return image, target\n",
    "        except (OSError, Image.DecompressionBombError) as e:\n",
    "            print(f\"âš ï¸ Skipping corrupt image: {path}\")\n",
    "            return self.__getitem__((index + 1) % len(self.samples))  # Skip and move to next\n",
    "\n",
    "# âœ… Load datasets\n",
    "train_dataset = CustomImageDataset(root=\"CropMonitoringData_Sorted/train\", transform=transform)\n",
    "val_dataset = CustomImageDataset(root=\"CropMonitoringData_Sorted/val\", transform=transform)\n",
    "\n",
    "# âœ… Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "print(f\"âœ… Train DataLoader: {len(train_loader)} batches\")\n",
    "print(f\"âœ… Val DataLoader: {len(val_loader)} batches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f38ba86b-2766-4672-a800-af05f4298004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Train DataLoader: 380 batches\n",
      "âœ… Val DataLoader: 95 batches\n"
     ]
    }
   ],
   "source": [
    "# âœ… Create DataLoaders (Set num_workers=0 for macOS compatibility)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"âœ… Train DataLoader: {len(train_loader)} batches\")\n",
    "print(f\"âœ… Val DataLoader: {len(val_loader)} batches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1435c075-079c-4de2-822a-81871762f372",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  27%|â–Œ | 101/380 [00:25<01:10,  3.96it/s, accuracy=92.5, loss=0.059]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy77_.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  52%|â–ˆ | 199/380 [00:50<00:44,  4.08it/s, accuracy=94.6, loss=0.176]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy76_.jpg\n",
      "âš ï¸ Skipping corrupt image: CropMonitoringData_Sorted/train/Healthy/healthy77_.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  62%|â–Œ| 236/380 [00:59<00:36,  3.94it/s, accuracy=95.1, loss=0.0103]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 27\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     29\u001b[0m correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (predicted \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 10  # You can adjust this\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# âœ… Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for images, labels in loop:\n",
    "        images, labels = images.to(device), labels.to(device)  # Move to MPS\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        loop.set_postfix(loss=loss.item(), accuracy=100 * correct / total)\n",
    "    \n",
    "    train_losses.append(running_loss / len(train_loader))\n",
    "    print(f\"âœ… Epoch {epoch+1} complete. Loss: {train_losses[-1]:.4f}\")\n",
    "\n",
    "print(\"ğŸ‰ Training finished successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c5377963-dd40-4e7a-ab2b-59ec34324db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model Accuracy on Validation Set: 98.58%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "accuracy = correct / total * 100\n",
    "print(f\"âœ… Model Accuracy on Validation Set: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e99ada96-4839-4580-9792-870ad6a696fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"crop_disease_classifier.pth\")\n",
    "print(\"âœ… Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02d9b981-b12e-4acb-9845-143f67032ec6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m      5\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_path)\n\u001b[1;32m      7\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      8\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)),\n\u001b[1;32m      9\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     10\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize([\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], [\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])\n\u001b[1;32m     11\u001b[0m ])\n\u001b[0;32m---> 12\u001b[0m image \u001b[38;5;241m=\u001b[39m transform(image)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[43mdevice\u001b[49m)\n\u001b[1;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "img_path = \"/Volumes/RAY/LeafSamples/leaf2.jpg\"  # Update with actual image path\n",
    "image = Image.open(img_path)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(image)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "\n",
    "print(f\"ğŸŸ¢ Predicted Class: {train_dataset.classes[predicted.item()]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cffa266-ed3a-4c29-990e-320db9d0c4ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
